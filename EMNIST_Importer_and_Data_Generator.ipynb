{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jonathan's EMNIST dataset importer + expander\n",
        "Welcome!\n",
        "\n",
        "This notebook imports the EMNIST dataset and provides tools to save them as files, either as images or as tensors.\n",
        "\n",
        "In addition, it includes the functionality to use `.ttf` (TrueType Font) files to generate more images, such as to add symbols to the dataset. Font files are linked in this projects' Github repository, to use them unzip the folder into `/MyDrive/Fonts/`\n",
        "\n",
        "EMNIST *(Expanded Modified National Institute of Standards and Technology database)* is a set of handwritten letters and numbers provided by NIST, which will be used to train a neural network in the next notebook."
      ],
      "metadata": {
        "id": "XoaG2W22J7uI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "dqV9EtBnKIGR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBBZn8fgVk8s"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image as im\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIL5dpOHWycJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67593ac0-7d60-40ce-dcf9-94ff3942d016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emnist\n",
        "from emnist import list_datasets, extract_training_samples, extract_test_samples"
      ],
      "metadata": {
        "id": "YO6jw5m6zZb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11281b47-e776-40af-cf3f-94878e7176cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emnist\n",
            "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from emnist) (1.22.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from emnist) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from emnist) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2022.12.7)\n",
            "Installing collected packages: emnist\n",
            "Successfully installed emnist-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Settings"
      ],
      "metadata": {
        "id": "Lxfeug5yKLXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell controls the location of the project's parent directory."
      ],
      "metadata": {
        "id": "WGr0Ik8ZKV7z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neqK5ZC7Z_uy"
      },
      "outputs": [],
      "source": [
        "dir = \"/content/drive\" + \"/MyDrive/Datasets/EMNIST\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, training data `x` and classes `y`, as well as testing data `x` and classes `y` are imported as images.\n",
        "\n",
        "The dataset's classes come as integers 0-46, so `key` is provided with the real class names."
      ],
      "metadata": {
        "id": "Cf4tBfZqK92U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2EenWHIrc0yg"
      },
      "outputs": [],
      "source": [
        "x, y = extract_training_samples('balanced')\n",
        "xt, yt = extract_test_samples('balanced')\n",
        "key = ['0','1','2','3','4','5','6','7','8','9','A','B','c','D','E','F','G','H','i','j','k','l','m','N','o','p','Q','R','s','T','u','v','w','x','y','z','a','b','d','e','f','g','h','n','q','r','t']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell iterates through class lists `y` and `yt` and creates new class lists `a` and `at`.\n",
        "\n",
        "While the old lists use integers for their classes, `a` and `at` use characters by indexing `key`."
      ],
      "metadata": {
        "id": "79e08q8tLbUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWlVb-snfg95"
      },
      "outputs": [],
      "source": [
        "a, at = [],[]\n",
        "for i in range(len(y)):\n",
        "  a.append(key[y[i]])\n",
        "for i in range(len(yt)):\n",
        "  at.append(key[yt[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Enrichment\n",
        "This section uses TrueType fonts to generate additional classes for the dataset."
      ],
      "metadata": {
        "id": "zTESfuNGYJOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import ImageFont, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil"
      ],
      "metadata": {
        "id": "2xfbTV9EUD9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the settings for the generator:\n",
        "- `pkey` is the list of characters to be generated.\n",
        "- `qkey` is the list of subdirectories to make (note `/` and `.` are different due to folder limitations).\n",
        "- `shrink` is the list of characters' images to make smaller if a full 28x28 space is too large for them.\n",
        "- `shrink2` is the list of characters' images to further reduce."
      ],
      "metadata": {
        "id": "ToiGeQomOk2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pkey = ['!','?','(',')',\"'\",'&','+','-','*','@','%','=',',',':',';','#','$','\\\\','/','.']\n",
        "qkey = ['!','?','(',')',\"'\",'&','+','-','*','@','%','=',',',':',';','#','$','\\\\',' ','~.']\n",
        "shrink = ['-','*','=','+',':',';']\n",
        "shrink2 = [',','.',\"'\"]"
      ],
      "metadata": {
        "id": "X6tyUWYijPBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To ensure proper use, this cell will display all font files present in the folder."
      ],
      "metadata": {
        "id": "WKxBboAtQEnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[x.path for x in os.scandir('/content/drive/MyDrive/Fonts')]"
      ],
      "metadata": {
        "id": "2NWYP7Jh54PG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell, for every font provided, creates an image for every character in `pkey`.\n",
        "\n",
        "It works with 7 steps per image made:\n",
        "1. A 100x100 blank canvas is created as a `numpy` array, saved to `image1`\n",
        "2. It is then converted to a PIL image object, saved to `image2`\n",
        "3. A PIL `ImageDraw` object is created for the image and drawn to `image2`, with a font size 56\n",
        "4. By using both `Image.crop()` and `Image.getbbox()`, the image is cropped to content and saved to `image3`\n",
        "5. The image is then resized such that its largest dimension is 26 pixels and saved to `image4`\n",
        "6. Finally, the image is padded to make it 28x28 and saved to `image5`\n",
        "7. For characters that are on the `shrink` or `shrink2` list, they are rescaled to 16x16 or 8x8 respectively and overwrite `image5`\n",
        "\n",
        "All generated data is stored in the `images` array. Since they are already ordered by class, a separate class list is not needed."
      ],
      "metadata": {
        "id": "2Yo4G7e7QR6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "for i in tqdm([x.path for x in os.scandir('/content/drive/MyDrive/Fonts')]):\n",
        "  font = ImageFont.truetype(i,size=56)\n",
        "  for j in range(len(pkey)):\n",
        "    image1 = np.zeros((100,100,3)).astype(np.uint8)\n",
        "    image2 = im.fromarray(image1)\n",
        "\n",
        "    draw = ImageDraw.Draw(image2)\n",
        "    draw.text(xy=(0,0),text=pkey[j],font=font)\n",
        "\n",
        "    image3 = np.asarray(image2.crop(image2.getbbox()))\n",
        "\n",
        "    if image3.shape[0] >= image3.shape[1]: # for tall images\n",
        "      i4x = int(image3.shape[1]*(26/image3.shape[0]))\n",
        "      i4x = i4x if i4x % 2 == 0 else i4x + 1\n",
        "      image4 = cv2.resize(image3,(i4x,26))\n",
        "\n",
        "      pad = int((28-image4.shape[1])/2)\n",
        "      image5 = cv2.copyMakeBorder(image4,1,1,pad,pad,cv2.BORDER_CONSTANT)\n",
        "\n",
        "    else: # for wide images\n",
        "      i4x = int(image3.shape[0]*(26/image3.shape[1]))\n",
        "      i4x = i4x if i4x % 2 == 0 else i4x + 1\n",
        "      image4 = cv2.resize(image3,(26,i4x))\n",
        "\n",
        "      pad = int((28-image4.shape[0])/2)\n",
        "      image5 = cv2.copyMakeBorder(image4,pad,pad,1,1,cv2.BORDER_CONSTANT)\n",
        "\n",
        "    if any(k == pkey[j] for k in shrink): # shrink\n",
        "      image5 = cv2.resize(image5,(16,16))\n",
        "      image5 = cv2.copyMakeBorder(image5,6,6,6,6,cv2.BORDER_CONSTANT)\n",
        "\n",
        "    if any(k == pkey[j] for k in shrink2): # shrink2\n",
        "      image5 = cv2.resize(image5,(8,8))\n",
        "      image5 = cv2.copyMakeBorder(image5,10,10,10,10,cv2.BORDER_CONSTANT)\n",
        "\n",
        "    images.append(image5)"
      ],
      "metadata": {
        "id": "QaTMGFsk3FH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e48b1fe-f2f3-4498-fd34-fc29881b6771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [00:07<00:00, 18.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tensor Format\n",
        "This section moves the dataset into tensors which are then saved in bulk. This is the method which will be used in the next notebook."
      ],
      "metadata": {
        "id": "QxYECht-npco"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define subdirectory names for raw (not expanded) and expanded datasets."
      ],
      "metadata": {
        "id": "WhFr75eNSrqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = 'raw'\n",
        "expanded = 'data'"
      ],
      "metadata": {
        "id": "nDscdoJ9Snac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two cells convert the:\n",
        "1. EMNIST training data\n",
        "2. EMNIST testing data\n",
        "3. Generated training data\n",
        "4. Generated testing data\n",
        "\n",
        "Into tensors, respectively.\n",
        "\n",
        "To emulate 3-channels, the images are stacked before being added to tensors."
      ],
      "metadata": {
        "id": "2bIE9R2vSzX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtor = torch.from_numpy(np.stack((x,x,x),axis=1))\n",
        "xttor = torch.from_numpy(np.stack((xt,xt,xt),axis=1))"
      ],
      "metadata": {
        "id": "UVWzj_rPLRxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imtor = torch.from_numpy(np.array(images[:2400]).transpose(0,3,1,2))\n",
        "imttor = torch.from_numpy(np.array(images[2400:]).transpose(0,3,1,2))"
      ],
      "metadata": {
        "id": "Sq2WvsbLKPHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell generates the class lists for the generated images. Since they are being saved to files, they will use integer classes."
      ],
      "metadata": {
        "id": "Y647CbX2T7m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imindex = []\n",
        "for i in range(2800):\n",
        "  imindex.append(i%len(qkey)+47)\n",
        "imindex = np.array(imindex)"
      ],
      "metadata": {
        "id": "TbRNd_V5NQeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next three cells combine the raw and generated data into training and testing image tensors.\n",
        "\n",
        "Then, it creates the training and testing class integers tensors as well as an array of the class names."
      ],
      "metadata": {
        "id": "yy0ymUf7UN9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_tensor = torch.cat((xtor, imtor), 0)\n",
        "test_image_tensor = torch.cat((xttor, imttor), 0)"
      ],
      "metadata": {
        "id": "smR-Ne8_MgLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_keys = torch.from_numpy(np.concatenate((y,imindex[:2400])))\n",
        "test_class_keys = torch.from_numpy(np.concatenate((yt,imindex[2400:])))"
      ],
      "metadata": {
        "id": "dDN-uw9aOMOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = np.array(key+pkey)"
      ],
      "metadata": {
        "id": "osoMi8itPT7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell saves the combined data set into the `data` subdirectory"
      ],
      "metadata": {
        "id": "3OTL_y0zVVDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name,var in zip(['train_data','test_data','train_keys','test_keys','class_names'],[train_image_tensor, test_image_tensor, train_class_keys, test_class_keys, class_names]):\n",
        "  torch.save(var,dir+'/data/'+name+'.dmp')"
      ],
      "metadata": {
        "id": "OB7x__EhQ3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Raw EMNIST Tensors"
      ],
      "metadata": {
        "id": "asad_V8m-G_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without the combining, it only takes these two cells to create the tensor files for the raw EMNIST data."
      ],
      "metadata": {
        "id": "s1hBtNVoVckX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_emnist_tensor = torch.from_numpy(np.stack((x,x,x),axis=1))\n",
        "test_emnist_tensor = torch.from_numpy(np.stack((xt,xt,xt),axis=1))\n",
        "train_emnist_keys = torch.from_numpy(y)\n",
        "test_emnist_keys = torch.from_numpy(yt)\n",
        "emnist_names = np.array(key)"
      ],
      "metadata": {
        "id": "ctgfRIEk-Kpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name,var in zip(['train_data','test_data','train_keys','test_keys','class_names'],[train_emnist_tensor, test_emnist_tensor, train_emnist_keys, test_emnist_keys, emnist_names]):\n",
        "  torch.save(var,dir+'/raw/'+name+'.dmp')"
      ],
      "metadata": {
        "id": "caCosAaz-lew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset as Images\n",
        "This section saves the dataset as individual images for each entry, organized into folders by class. This approach is not recommended."
      ],
      "metadata": {
        "id": "qaa5tqe0KhXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines subdirectory names for training and testing data."
      ],
      "metadata": {
        "id": "-rvi3fG4K2NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = \"train\"\n",
        "test = \"valid\""
      ],
      "metadata": {
        "id": "cJ34n-S8K4jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ease of use, this cell creates directories for all classes inside the training and testing subdirectories."
      ],
      "metadata": {
        "id": "ozXXBLI7LzST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KzqVMsYn6aR"
      },
      "outputs": [],
      "source": [
        "for i in range(len(key)):\n",
        "  os.mkdir(dir+'/'+train+'/'+key[i])\n",
        "  os.mkdir(dir+'/'+test+'/'+key[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next two cells save the training and testing data, respectively, into their corresponding folders.\n",
        "\n",
        "Since these are saving large numbers of files to Google Drive, they may take several minutes to run."
      ],
      "metadata": {
        "id": "qas4_n0HL__k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtGG1pn8dMET"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "for i in tqdm(np.arange(start,len(x))):\n",
        "  im.fromarray(x[i]).save(dir+'/'+train+'/'+a[i]+'/'+str(i)+'.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-isRyz7IptO6"
      },
      "outputs": [],
      "source": [
        "start = 0\n",
        "for i in tqdm(np.arange(start,len(xt))):\n",
        "  im.fromarray(xt[i]).save(dir+'/'+test+'/'+at[i]+'/'+str(i)+'.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enriched Data Saving"
      ],
      "metadata": {
        "id": "5FVyEd05PYnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ease of use, the next two cells can generate and delete the subdirectories for enriched data, respectively.\n",
        "\n",
        "Directory deletion is provided in the case the dataset must be changed."
      ],
      "metadata": {
        "id": "TaCApCl_PgwD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(qkey)): #generate directories\n",
        "  os.mkdir(dir+'/'+train+'/'+qkey[i])\n",
        "  os.mkdir(dir+'/'+test+'/'+qkey[i])"
      ],
      "metadata": {
        "id": "DCsacRzYPdHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for i in range(len(qkey)): #delete directories\n",
        "#  shutil.rmtree(dir+'/'+train+'/'+qkey[i])\n",
        "#  shutil.rmtree(dir+'/'+test+'/'+qkey[i])"
      ],
      "metadata": {
        "id": "t6CdB7uzPdSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saves the enriched data and splits between training and testing"
      ],
      "metadata": {
        "id": "XlfOKEc7P03S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(images))):\n",
        "  if i < (len(images)*6/7):\n",
        "    im.fromarray(images[i]).save(dir+'/'+train+'/'+qkey[i%len(qkey)]+'/'+str(i)+'.jpg')\n",
        "  else:\n",
        "    im.fromarray(images[i]).save(dir+'/'+test+'/'+qkey[i%len(qkey)]+'/'+str(i)+'.jpg')"
      ],
      "metadata": {
        "id": "rvYB8xJgP0U8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}