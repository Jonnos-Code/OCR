{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Jonathan's EMNIST Neural Network trainer\n",
        "---\n",
        "Welcome!\n",
        "\n",
        "This notebook uses the data exported from the previous notebook to train a neural network and save it, which will then be used in the next notebook as the core of an OCR program.\n",
        "\n",
        "*Note:\n",
        "This was created for the tensor-format exported data. Dataset as images will not work in this notebook, and are extremely slow when they do work. Use of that format is not advised.*\n",
        "\n",
        "**Run this notebook with GPU hardware acceleration if possible**"
      ],
      "metadata": {
        "id": "CRZbHI1sBl7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "1-wUOlwGnDNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnB74Kj5ifPL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms, io\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
        "from torchsummary import summary\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import copy\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Tc0cJ3TZii2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6287204-968f-457c-999c-d2e7523bf024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup & Settings\n",
        "Define file paths and adjust Dataloader settings if needed"
      ],
      "metadata": {
        "id": "nUnS2KthCh5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define parent directory of project"
      ],
      "metadata": {
        "id": "xTBTvIYbClIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Datasets/EMNIST/'"
      ],
      "metadata": {
        "id": "2ZHGP3Sjkhnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`src` refers to the data source. If default folder names were used in the preceding notebook, use `raw` for the normal EMNIST format and `data` for enriched data."
      ],
      "metadata": {
        "id": "beFffxalCqVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = 'raw' # 'raw' for pure EMNIST, 'data' for my punctuation-expanded EMNIST\n",
        "train_data = torch.load(dir+src+'/train_data.dmp').float()/255\n",
        "test_data = torch.load(dir+src+'/test_data.dmp').float()/255\n",
        "train_keys = torch.load(dir+src+'/train_keys.dmp')\n",
        "test_keys = torch.load(dir+src+'/test_keys.dmp')\n",
        "class_names = torch.load(dir+src+'/class_names.dmp')"
      ],
      "metadata": {
        "id": "NzBinkEJViuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'train':train_data,'valid':test_data}\n",
        "keys = {'train':train_keys,'valid':test_keys}"
      ],
      "metadata": {
        "id": "_n4XWsphehQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to allow the data to be used, I had to create my own PyTorch Utilities Dataset class. It returns a dataset object using data and key tensors as inputs."
      ],
      "metadata": {
        "id": "SMS1fYRHC-IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class JDataset(Dataset):\n",
        "  def __init__(self,data,labels):\n",
        "    if torch.is_tensor(data) and torch.is_tensor(labels):\n",
        "      if data.shape[0] == labels.shape[0]:\n",
        "        self.samples = data\n",
        "        self.labels = labels\n",
        "      else:\n",
        "        raise Exception('Samples and labels not same length')\n",
        "    else:\n",
        "      raise Exception('Samples and labels must both be tensors')\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "  def __getitem__(self,index):\n",
        "    data = self.samples[index]\n",
        "    label = self.labels[index]\n",
        "    return data, label"
      ],
      "metadata": {
        "id": "W7E7kH5TZJm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell uses the defined JDataset class to create training and testing data."
      ],
      "metadata": {
        "id": "2VJupjK7DSYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jdataset = {x: JDataset(data[x],keys[x]) for x in ['train','valid']}"
      ],
      "metadata": {
        "id": "oqXNMhXeflSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the settings for the dataloader. `batch` refers to size of a single batch that gets fed into the neural network, and `batches` defines how many are fed to it in a single iteration. I recommend `batch` and `batches` be set to multiply together to equal the amount of data in that set.\n",
        "\n",
        "---\n",
        "\n",
        "**Recommended Settings:**\n",
        "\n",
        "- Raw Data:\n",
        " - `batches = {'train':300, 'valid':50}`\n",
        " - `batch = {'train':376, 'valid':376}`\n",
        "- Default Enriched Data (140 fonts, 20 characters/font):\n",
        " - `batches = {'train':384, 'valid':64}`\n",
        " - `batch = {'train':300, 'valid':300}`"
      ],
      "metadata": {
        "id": "MbLeCbOBDXuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batches = {'train': 384, 'valid': 64}\n",
        "batch = {'train': 300, 'valid': 300}"
      ],
      "metadata": {
        "id": "K2XI82TqVJK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up the weighted random sampler and the dataloader. The loader is used to provide tensors of data to be fed to the neural network, and the weighted random sampler is used to ensure equal representation for enriched data which is smaller by default."
      ],
      "metadata": {
        "id": "Snd3r-iGFKg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = {x: 1./np.array(np.unique(jdataset[x].labels, return_counts=True)[1]) for x in ['train','valid']}\n",
        "sample_weights = {x: torch.from_numpy(weights[x][jdataset[x].labels]) for x in ['train','valid']}\n",
        "\n",
        "Sampler = {x: WeightedRandomSampler(sample_weights[x], batch[x]*batches[x]) for x in ['train','valid']}\n",
        "dataloader = {x: DataLoader(jdataset[x], batch_size=batch[x], num_workers = 1, sampler=Sampler[x], pin_memory=True) for x in ['train','valid']}"
      ],
      "metadata": {
        "id": "Ph2kPdlPgHYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Device definition (please use a GPU!)"
      ],
      "metadata": {
        "id": "pnkiocnDFhu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #use GPU cores if available, else cpu\n",
        "dataset_sizes = {x: len(jdataset[x]) for x in ['train','valid']} #lengths of datasets"
      ],
      "metadata": {
        "id": "_qfSeeWXisrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Preparation\n",
        "\n",
        "This is needed because the standard Alexnet takes 224x224x3 images, while for the purposes of my OCR, it needs to take the 28x28 images provided by EMNIST. While the images can be stacked to emulate 3 color channels, the model needs adjustment to fit their small size."
      ],
      "metadata": {
        "id": "HDWid_YXnF8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import empty Alexnet"
      ],
      "metadata": {
        "id": "vLlQLA1nGUDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alexnet = models.alexnet(weights = None)"
      ],
      "metadata": {
        "id": "lRLvyffRnH2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjust to fit my data;\n",
        "\n",
        "This deletes two convolution layers and one max pooling layer from Alexnet. The remaining convolutional layers are adjusted with smaller kernels and more padding to ensure they don't remove more data than they process. The final layer is edited to fit all 67 classes."
      ],
      "metadata": {
        "id": "97VKPQHEGWMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jnet = copy.deepcopy(alexnet)\n",
        "del(jnet.features[5:10])\n",
        "jnet.features[0] = nn.Conv2d(3, 32, kernel_size=(5,5), stride=(1,1), padding='same')\n",
        "jnet.features[3] = nn.Conv2d(32, 64, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "jnet.features[5] = nn.Conv2d(64, 256, kernel_size=(3,3), stride=(1,1), padding='same')\n",
        "jnet.classifier[6] = nn.Linear(4096, 67)\n",
        "jnet = jnet.to(device)"
      ],
      "metadata": {
        "id": "jWjEs7r1rQji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(jnet,(3,28,28))"
      ],
      "metadata": {
        "id": "RbqJVM8Oo2xQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c141a862-5662-42d9-9c68-5e9f9de180d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]           2,432\n",
            "              ReLU-2           [-1, 32, 28, 28]               0\n",
            "         MaxPool2d-3           [-1, 32, 13, 13]               0\n",
            "            Conv2d-4           [-1, 64, 13, 13]          18,496\n",
            "              ReLU-5           [-1, 64, 13, 13]               0\n",
            "            Conv2d-6          [-1, 256, 13, 13]         147,712\n",
            "              ReLU-7          [-1, 256, 13, 13]               0\n",
            "         MaxPool2d-8            [-1, 256, 6, 6]               0\n",
            " AdaptiveAvgPool2d-9            [-1, 256, 6, 6]               0\n",
            "          Dropout-10                 [-1, 9216]               0\n",
            "           Linear-11                 [-1, 4096]      37,752,832\n",
            "             ReLU-12                 [-1, 4096]               0\n",
            "          Dropout-13                 [-1, 4096]               0\n",
            "           Linear-14                 [-1, 4096]      16,781,312\n",
            "             ReLU-15                 [-1, 4096]               0\n",
            "           Linear-16                   [-1, 67]         274,499\n",
            "================================================================\n",
            "Total params: 54,977,283\n",
            "Trainable params: 54,977,283\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.62\n",
            "Params size (MB): 209.72\n",
            "Estimated Total Size (MB): 211.35\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Once everything else is prepared, get in here to start training the model!"
      ],
      "metadata": {
        "id": "Iv5EGj-1vtly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the primary training loop. Feel free to adjust the settings to your liking to fiddle with how it learns. Running this cell multiple times continues training the same model, although the preparation section can be run again if you wish to start over.\n",
        "\n",
        "Settings:\n",
        "\n",
        "- `num_epochs` - The number of iterations to train the neural network before stopping. Be careful not to train too much to avoid overfitting!\n",
        "- `lr` - Learning rate. This adjusts the rate at which the model's parameters can change. Too low values make slow training, but high values make less precise training. I've found 0.005 works well, but feel free to change it.\n",
        "- `momentum` - The amount of influence previous iterations have on future training. I haven't tried changing this but 0.9 works well, as it ensures training is consistent but allows room to move if needed."
      ],
      "metadata": {
        "id": "wrSRM31vGq9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(jnet.parameters(), lr=0.005, momentum=0.9)\n",
        "num_epochs = 1000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  jnet.train()\n",
        "  traincorrect = 0\n",
        "  for inputs, labels in dataloader['train']:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = jnet(inputs)\n",
        "    preds = torch.max(outputs,1)[1]\n",
        "    traincorrect += torch.sum(preds == labels.data)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(outputs,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  jnet.eval()\n",
        "  testcorrect = 0\n",
        "  for inputs, labels in dataloader['valid']:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = jnet(inputs)\n",
        "    preds = torch.max(outputs, 1)[1]\n",
        "    testcorrect += torch.sum(preds == labels.data)\n",
        "\n",
        "  print('Epoch {:03d}'.format(epoch+1),'-'*15,'Training Accuracy: {:.2f}%'.format((traincorrect*100)/(batch['train']*batches['train'])),'-'*5,'Testing Accuracy: {:.2f}%'.format((testcorrect*100)/(batch['valid']*batches['valid'])))"
      ],
      "metadata": {
        "id": "wbDkULVcwhb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "51c984eb-6c71-43a1-fc96-fcfe1e3c2d3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 --------------- Training Accuracy: 51.13% ----- Testing Accuracy: 74.01%\n",
            "Epoch 002 --------------- Training Accuracy: 76.30% ----- Testing Accuracy: 81.65%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-05817c863fc8>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mtraincorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving & Loading\n",
        "To save this model for use, and to load it here if you wish to train the same model more, use this section."
      ],
      "metadata": {
        "id": "XY0DTja4BrI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`this_model` refers to the filepath to write to/load model from"
      ],
      "metadata": {
        "id": "MCFFjr0GIJND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "this_model = 'saved models/jnet.dat'"
      ],
      "metadata": {
        "id": "Y0T3Rn7vH73D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(jnet.state_dict(),dir+this_model)"
      ],
      "metadata": {
        "id": "DP-k1RoGUS5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jnet.load_state_dict(torch.load(dir+this_model))"
      ],
      "metadata": {
        "id": "9ts1xQM8Uxn4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}